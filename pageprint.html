<html>

<body style="background-color: aqua;">
    <h1>Information technology:-</h1>
    <p>Information technology (IT) is the use of computers to create, process, store, retrieve and exchange all kinds of
        data[1] and information. IT forms part of information and communications technology (ICT).[2] An information
        technology system (IT system) is generally an information system, a communications system, or, more specifically
        speaking, a computer system — including all hardware, software, and peripheral equipment — operated by a limited
        group of IT users, and an IT project usually refers to the commissioning and implementation of an IT
        system.[3]although humans have been storing, retrieving, manipulating, and communicating information since the
        earliest information distribution technologies such as television and telephones. Several products or services
        within an
        economy are associated with information technology, including computer hardware, software, electronics,
        semiconductors, internet, telecom equipment, and e-commerce.[6][a] based on the storage and processing
        technologies employed, it is possible to distinguish four distinct phases of
        IT development: pre-mechanical (3000 BC — 1450 AD), mechanical (1450 — 1840), electromechanical (1840 — 1940),
        and electronic (1940 to present).[4] Information technology is also a branch of computer science, which can be
        defined as the overall study of procedure, structure, and the processing of various types of data. As this field
        continues to evolve across the
        world, its overall priority and importance has also grown, which is where we begin to see the introduction of
        computer science-related courses in K-12 education Zuse Z3 replica on display at Deutsches Museum in Munich. The
        Zuse Z3 is the first programmable computer.Main article: History of computing hardwar their efforts were focused
        on designing the first digital computer. Along with that, topics such as artificial
        intelligence began to be brought up as Turing was beginning to question such technology of the time period.[9]
        Devices have been used to aid computation for thousands of years, probably initially in the form of a tally
        stick.[10] The Antikythera mechanism, dating from about the beginning of the first century BC, is generally
        considered to be the earliest known mechanical analog computer, and the earliest known geared mechanism.[11]
        Comparable geared devices did not emerge in Europe until the 16th century, and it was not until 1645 that the
        first mechanical calculator capable of performing the four basic arithmetical operations was developed.[12]
    </p>
    <h1>Electronic computers:-</h1>
    <p>
        Electronic computers, using either relays or valves, began to appear in the early 1940s. The electromechanical
        Zuse Z3, completed in 1941, was the world's first programmable computer, and by modern standards one of the
        first machines that could be considered a complete computing machine. During the Second World War, Colossus
        developed the first electronic digital computer to decrypt German messages. Although it was programmable, it was
        not general-purpose, being designed to perform only a single task. It also lacked the ability to store its
        program in memory; programming was carried out using plugs and switches to alter the internal wiring.[13] The
        first recognizably modern electronic digital stored-program computer was the Manchester Baby, which ran its
        first program on 21 June 1948.[14]The development of transistors in the late 1940s at Bell Laboratories allowed
        a new generation of computers to be designed with greatly reduced power consumption. The first commercially
        available stored-program computer, the Ferranti Mark I, contained 4050 valves and had a power consumption of 25
        kilowatts. By comparison, the first transist orized computer developed at the University of Manchester and
        operational by November 1953, consumed only 150 watts in its final version.[15]Several other breakthroughs in
        semiconductor technology include the integrated circuit (IC) invented by Jack Kilby at Texas Instruments and
        Robert Noyce at Fairchild Semiconductor in 1959, the metal field-effect transistor (MOSFET) invented by Mohamed
        Atalla and Dawon Kahng at Bell Laboratories in 1959, and
        the microprocessor invented by Ted Hoff, Federico Faggin, Masatoshi Shima, and Stanley Mazor at Intel in 1971.
        These important inventions led to the development of the personal computer (PC) in the 1970s, and the emergence
        of information and communications technology (ICT).[16]
    </p>
    <button onclick="display()" style="font-size: 20px; color: #ff0000; background-color: chartreuse;">Click to
        Print</button>
    <script>
        function display() {
            window.print();
        }
    </script>
</body>